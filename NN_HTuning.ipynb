{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network hyperparameter tuning "
      ],
      "metadata": {
        "id": "ldUfk8ujgArb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section of the project the NN will be defined with their optimal hyperparameters using Keras Tuner (https://keras.io/keras_tuner/).\n",
        "There will be one neural network for each subset of features (DataAP, dataPHY and dataALL)"
      ],
      "metadata": {
        "id": "AMn81UhUgNFX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9NxpD1fJiy9"
      },
      "source": [
        "## Loading the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjk9ZGgWGnlg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import pickle as pk\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coZ-triGJow-"
      },
      "source": [
        "## Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tjVE5BAGbxc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18efcbac-93ec-44a8-f345-c070e8305a72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVFgplJbcQKs"
      },
      "source": [
        "Importing the dataframes dataAP and dataPHY:\n",
        "\n",
        "- dataAP: data containing the original variables that best correlate with the label data of the dataframe.\n",
        "\n",
        "- dataPHY: data containing variables from the blood test that are of interest.\n",
        "\n",
        "- dataALL: Both dataAP and dataPHY combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdSuIyrHf1_K"
      },
      "outputs": [],
      "source": [
        "#Importing dataAP\n",
        "file_name = '/content/drive/MyDrive/TFG/dataAP.csv'\n",
        "dataAP = pd.read_csv(file_name, index_col=[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPMXUik7THTH"
      },
      "outputs": [],
      "source": [
        "#Importing dataPHY\n",
        "file_name = '/content/drive/MyDrive/TFG/dataPHY.csv'\n",
        "dataPHY = pd.read_csv(file_name, index_col=[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Djwv3OgzsbDq"
      },
      "outputs": [],
      "source": [
        "#Importing dataALL\n",
        "file_name = '/content/drive/MyDrive/TFG/dataALL.csv'\n",
        "dataALL = pd.read_csv(file_name, index_col=[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4851cmvA1CZ"
      },
      "source": [
        "One dataframe (**dataAP_g**) will contain the label data in continuous form (Masa_VAT_g) and the other (**dataAP_cat**) will contain the label in\n",
        "categorical form (Masa_VAT_cat), as stated in the \"*Transorming the label data into categotical data*\" section from the preprocessing. This is done to avoid confusion when selecting the target variable (Masa_VAT_cat).\n",
        "\n",
        "Also the label Vol_VAT will be removed as it is technically the same as Masa_VAT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBqWTiOiP5_G"
      },
      "outputs": [],
      "source": [
        "#Creating a dataAP dataframe only with Masa_VAT_g and another one with only Masa_VAT_cat\n",
        "\n",
        "dataAP_g = dataAP.copy()\n",
        "dataAP_cat = dataAP.copy()\n",
        "\n",
        "dropcat = [\"Masa_VAT_g\", \"Vol_VAT\"]\n",
        "dropg = [\"Masa_VAT_cat\", \"Vol_VAT\"]\n",
        "dataAP_g = dataAP_g.drop(dropg, axis=1)\n",
        "dataAP_cat = dataAP_cat.drop(dropcat , axis=1)\n",
        "\n",
        "#And deleting all the label data from dataALL\n",
        "dropALL = [\"Masa_VAT_cat\", \"Masa_VAT_g\", \"Vol_VAT\"]\n",
        "dataALL = dataALL.drop(dropALL, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFSaiI7vKO0Y"
      },
      "source": [
        "Separating the features X from the label y, next we have the three possible X and y depending on the database we're going to work with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYoO7xYqKUsT"
      },
      "outputs": [],
      "source": [
        "#DATA AP NN\n",
        "#Separating the target variables (Masa_VAT_g and Vol_VAT) from the independent features\n",
        "target = [\"Masa_VAT_g\"]\n",
        "X = dataAP_g.drop(target, axis=1).values\n",
        "y = dataAP_g[target].values\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hsz8LGMlcIJT"
      },
      "outputs": [],
      "source": [
        "#DATA PHY NN\n",
        "#Separating the target variables (Masa_VAT_g and Vol_VAT) from the independent features\n",
        "target = [\"Masa_VAT_g\"]\n",
        "X = dataPHY.values\n",
        "y = dataAP_g[target].values\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhWwiCCKggDP"
      },
      "outputs": [],
      "source": [
        "#DATA ALL NN\n",
        "#Separating the target variables (Masa_VAT_g and Vol_VAT) from the independent features\n",
        "target = [\"Masa_VAT_g\"]\n",
        "X = dataALL.values\n",
        "y = dataAP_g[target].values\t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqGXLdK9Kofh"
      },
      "source": [
        "Train-Validation-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJLsA57gKpDi"
      },
      "outputs": [],
      "source": [
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Now we can use X_train and y_train as your training data\n",
        "# and X_test and y_test as your test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pissLm6JKwle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e84166ee-c9e8-4f54-c4c2-84a75a4e8297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(118, 33)\n",
            "(118, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zE99rG20rFP"
      },
      "source": [
        "# Creating a model with keras tuner: DataAP"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the data used in the training and testing"
      ],
      "metadata": {
        "id": "KxdbctoKg4Aj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3tM8cIhye-e"
      },
      "outputs": [],
      "source": [
        "## Data AP\n",
        "#Separating the target variables (Masa_VAT_g and Vol_VAT) from the independent features\n",
        "target = [\"Masa_VAT_g\"]\n",
        "X = dataAP_g.drop(target, axis=1).values\n",
        "y = dataAP_g[target].values\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAbgU2jzye-p"
      },
      "outputs": [],
      "source": [
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Now we can use X_train and y_train as your training data\n",
        "# and X_test and y_test as your test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in0HByyiye-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92eaf63e-ee0f-4543-9c70-21258d923c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(118, 17)\n",
            "(118, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY56GaBDDUnj"
      },
      "source": [
        "Installing keras tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AR_NCnY81Ilv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3939bf27-caf5-4cf4-bd21-0c9bcdd66cf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.27.1)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaIn3A1p1QW3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fa82d1e-6eb8-45fa-b0d9-5d42512d4a3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-091a60f0002a>:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  from kerastuner import HyperModel\n"
          ]
        }
      ],
      "source": [
        "from kerastuner import HyperModel\n",
        "from kerastuner.tuners import RandomSearch, Hyperband\n",
        "import tensorflow as tf\n",
        "import IPython\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csJ8RakQ1WTS"
      },
      "outputs": [],
      "source": [
        "class ANNhypermodel(HyperModel):\n",
        "    \n",
        "    def __init__(self, input_shape):\n",
        "        self.input_shape= (17,)\n",
        "        \n",
        "    def build(self, hp):\n",
        "        model= tf.keras.Sequential()\n",
        "        \n",
        "        # Tune the number of units in the first Dense layer\n",
        "        # Defining dense units as a close approx to the original neural network to perform a fair comparision!\n",
        "        \n",
        "        \n",
        "        hp_units_1= hp.Int('units_1', min_value=128, max_value= 160, step=32)\n",
        "        hp_units_2= hp.Int('units_2', min_value=64, max_value= 128, step=32)\n",
        "\n",
        "        model.add(tf.keras.layers.Dense(units=hp_units_1, activation='relu', input_shape= self.input_shape))\n",
        "        model.add(tf.keras.layers.Dense(units=hp_units_2, activation='relu'))\n",
        "        model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "        # Tune the learning rate for the optimizer \n",
        "        hp_learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default= 0.0005)\n",
        "\n",
        "        model.compile(loss='mse',\n",
        "                    optimizer= tf.keras.optimizers.Adam(learning_rate=hp_learning_rate), metrics= ['mae','mse']\n",
        "                     )\n",
        "\n",
        "        return model\n",
        "\n",
        "hypermodel= ANNhypermodel(input_shape= [len(X_train)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7avhshkV1zkv"
      },
      "source": [
        "2. Instantiate the tuner to perform hypertuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U23iJS2F1zT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab111d1f-d718-4f20-d663-877398109683"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 3\n",
            "units_1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 128, 'max_value': 160, 'step': 32, 'sampling': 'linear'}\n",
            "units_2 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
            "learning_rate (Float)\n",
            "{'default': 0.0005, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
          ]
        }
      ],
      "source": [
        "MAX_TRIALS = 20\n",
        "EXECUTION_PER_TRIAL = 2\n",
        "tuner= RandomSearch(hypermodel,\n",
        "               objective= 'val_mse',\n",
        "               max_trials= MAX_TRIALS,\n",
        "               executions_per_trial= EXECUTION_PER_TRIAL,\n",
        "               directory= 'random_search',\n",
        "               project_name='houseprices',\n",
        "               overwrite=True)\n",
        "\n",
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5auRm8fh2WJO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca92ae79-e4d3-4898-db97-7dd921887df0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 3\n",
            "units_1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 128, 'max_value': 160, 'step': 32, 'sampling': 'linear'}\n",
            "units_2 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
            "learning_rate (Float)\n",
            "{'default': 0.0005, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
          ]
        }
      ],
      "source": [
        "HYPERBAND_MAX_EPOCHS = 150\n",
        "EXECUTION_PER_TRIAL = 2\n",
        "\n",
        "tuner= Hyperband(hypermodel,\n",
        "                   objective= 'val_mse',\n",
        "                   max_epochs=HYPERBAND_MAX_EPOCHS, #Set 100+ for good results\n",
        "                   executions_per_trial=EXECUTION_PER_TRIAL,\n",
        "                   directory= 'hyperband',\n",
        "                   project_name='houseprices',\n",
        "                   overwrite=True)\n",
        "\n",
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfEYafbQ2krn"
      },
      "source": [
        "Run the hyperparameter search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rb-lA2NU2ptn",
        "outputId": "8f9ce6fb-08f7-4d91-d8b6-8af9cc53cf84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "searching for the best params!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd7e81bf400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fd7e81bdd80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1281.1549594402313  secs\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "print('searching for the best params!')\n",
        "\n",
        "t0= time.time()\n",
        "tuner.search(x= X_train,\n",
        "             y= y_train,\n",
        "             epochs=100,\n",
        "             batch_size= 64,\n",
        "             validation_data= (X_test, y_test),\n",
        "             verbose=0,\n",
        "             callbacks= []\n",
        "            )\n",
        "print(time.time()- t0,\" secs\")\n",
        "\n",
        "# Retreive the optimal hyperparameters\n",
        "best_hps= tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Retrieve the best model\n",
        "best_model = tuner.get_best_models(num_models=1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZdURQwm3_s3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09082197-21aa-4142-a3b8-cd4f466f1798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The hyperparameter search is complete. The optimal number of units in the \n",
            "first densely-connected layer is 160,\n",
            "second layer is 64 \n",
            "\n",
            "and the optimal learning rate for the optimizer\n",
            "is 0.0012300102366624482.\n",
            "\n",
            "[]\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 0.0197 - mae: 0.1140 - mse: 0.0197\n",
            "loss:0.01966085098683834 mae: 0.1139756515622139 mse: 0.01966085098683834\n"
          ]
        }
      ],
      "source": [
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units in the \n",
        "first densely-connected layer is {best_hps.get('units_1')},\n",
        "second layer is {best_hps.get('units_2')} \n",
        "\n",
        "and the optimal learning rate for the optimizer\n",
        "is {best_hps.get('learning_rate')}.\n",
        "\"\"\")\n",
        "\n",
        "# Evaluate the best model.\n",
        "print(best_model.metrics_names)\n",
        "loss, mae, mse = best_model.evaluate(X_test, y_test)\n",
        "print(f'loss:{loss} mae: {mae} mse: {mse}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_waz7g45EbM"
      },
      "source": [
        "Retrain the model with the optimal hyperparameters from the search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocL8HSg35Fii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abab5f36-ed76-4e75-d8a3-d8c99d652597"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Using Early stopping, needed only 17 epochs to converge!\n"
          ]
        }
      ],
      "source": [
        "# Define the early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "# Build the model with the optimal hyperparameters\n",
        "tuned_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Train the model with early stopping\n",
        "t00 = time.time()\n",
        "history_tuned = tuned_model.fit(X_train, y_train, \n",
        "                                epochs=200, \n",
        "                                validation_data=(X_test, y_test),\n",
        "                                verbose=0,\n",
        "                                callbacks=[early_stopping])\n",
        "\n",
        "print(\"\\n Using Early stopping, needed only\", len(history_tuned.history['val_mse']), \"epochs to converge!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii8vNrMg5-8O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c92b1fec-96b5-4305-eff6-496c64e3bc88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Training accuracy:  0.3094036012944743\n",
            "Test accuracy:  0.29299216657310345\n",
            "Test mean-squared error:  0.15362595831903117\n"
          ]
        }
      ],
      "source": [
        "y_pred_train_tuned= tuned_model.predict(X_train).flatten()\n",
        "y_pred_test_tuned= tuned_model.predict(X_test).flatten()\n",
        "\n",
        "print(\"Training accuracy: \",r2_score(y_train, y_pred_train_tuned))\n",
        "\n",
        "print(\"Test accuracy: \",r2_score(y_test, y_pred_test_tuned))\n",
        "\n",
        "print(\"Test mean-squared error: \",np.sqrt(mean_squared_error(y_test, y_pred_test_tuned)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2aZvqRcqCc6"
      },
      "source": [
        "From the hyperparameter tuning we got that:\n",
        "\n",
        "The optimal number of units in the densely-connected layers is:\n",
        "- First layer: 128\n",
        "- Second layer: 64\n",
        "\n",
        "and the optimal learning rate for the optimizer\n",
        "is 0.009925419577869306.\n",
        "\n",
        "And using early stopping only 18 epochs were needed to converge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVJUegYayltZ"
      },
      "source": [
        "# Creating a model with keras tuner: DataPHY"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the data used in the training and testing"
      ],
      "metadata": {
        "id": "jO9PLmuchEeO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-T7-aClpyucX"
      },
      "outputs": [],
      "source": [
        "#DATA PHY NN\n",
        "#Separating the target variables (Masa_VAT_g and Vol_VAT) from the independent features\n",
        "target = [\"Masa_VAT_g\"]\n",
        "X = dataPHY.values\n",
        "y = dataAP_g[target].values\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDl2ozVNyucg"
      },
      "outputs": [],
      "source": [
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Now we can use X_train and y_train as your training data\n",
        "# and X_test and y_test as your test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqRgwLZFyuch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f433b86c-0a5e-441b-cfcb-3d9d3830a9fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(118, 16)\n",
            "(118, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3qGFWwkylta",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49218073-b8a5-4528-bf35-f33538f01467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.27.1)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DP_d7pcvylta"
      },
      "outputs": [],
      "source": [
        "from kerastuner import HyperModel\n",
        "from kerastuner.tuners import RandomSearch, Hyperband\n",
        "import tensorflow as tf\n",
        "import IPython\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNDDiFKsylta"
      },
      "outputs": [],
      "source": [
        "class ANNhypermodel(HyperModel):\n",
        "    \n",
        "    def __init__(self, input_shape):\n",
        "        self.input_shape= (16,)\n",
        "        \n",
        "    def build(self, hp):\n",
        "        model= tf.keras.Sequential()\n",
        "        \n",
        "        # Tune the number of units in the first Dense layer\n",
        "        # Defining dense units as a close approx to the original neural network to perform a fair comparision!\n",
        "        \n",
        "        \n",
        "        hp_units_1= hp.Int('units_1', min_value=128, max_value= 160, step=32)\n",
        "        hp_units_2= hp.Int('units_2', min_value=64, max_value= 128, step=32)\n",
        "\n",
        "        model.add(tf.keras.layers.Dense(units=hp_units_1, activation='relu', input_shape= self.input_shape))\n",
        "        model.add(tf.keras.layers.Dense(units=hp_units_2, activation='relu'))\n",
        "        model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "        # Tune the learning rate for the optimizer \n",
        "        hp_learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default= 0.0005)\n",
        "\n",
        "        model.compile(loss='mse',\n",
        "                    optimizer= tf.keras.optimizers.Adam(learning_rate=hp_learning_rate), metrics= ['mae','mse']\n",
        "                     )\n",
        "\n",
        "        return model\n",
        "\n",
        "hypermodel= ANNhypermodel(input_shape= [len(X_train)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50-A9pK0yltb"
      },
      "source": [
        "2. Instantiate the tuner to perform hypertuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SwAD4Ccyltb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05e91e0d-bf33-4cdc-ff4b-c1046b80ad09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 3\n",
            "units_1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 128, 'max_value': 160, 'step': 32, 'sampling': 'linear'}\n",
            "units_2 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
            "learning_rate (Float)\n",
            "{'default': 0.0005, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
          ]
        }
      ],
      "source": [
        "MAX_TRIALS = 20\n",
        "EXECUTION_PER_TRIAL = 2\n",
        "tuner= RandomSearch(hypermodel,\n",
        "               objective= 'val_mse',\n",
        "               max_trials= MAX_TRIALS,\n",
        "               executions_per_trial= EXECUTION_PER_TRIAL,\n",
        "               directory= 'random_search',\n",
        "               project_name='houseprices',\n",
        "               overwrite=True)\n",
        "\n",
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJ6osdpGyltb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74aac30c-d206-4adc-997a-4733a1ba8010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 3\n",
            "units_1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 128, 'max_value': 160, 'step': 32, 'sampling': 'linear'}\n",
            "units_2 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
            "learning_rate (Float)\n",
            "{'default': 0.0005, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
          ]
        }
      ],
      "source": [
        "HYPERBAND_MAX_EPOCHS = 150\n",
        "EXECUTION_PER_TRIAL = 2\n",
        "\n",
        "tuner= Hyperband(hypermodel,\n",
        "                   objective= 'val_mse',\n",
        "                   max_epochs=HYPERBAND_MAX_EPOCHS, #Set 100+ for good results\n",
        "                   executions_per_trial=EXECUTION_PER_TRIAL,\n",
        "                   directory= 'hyperband',\n",
        "                   project_name='houseprices',\n",
        "                   overwrite=True)\n",
        "\n",
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt3Tx9Yayltb"
      },
      "source": [
        "Run the hyperparameter search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn04o6WPyltb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "184d1865-4049-46d1-e7de-d3309aa627c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "searching for the best params!\n",
            "1338.1303160190582  secs\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "print('searching for the best params!')\n",
        "\n",
        "t0= time.time()\n",
        "tuner.search(x= X_train,\n",
        "             y= y_train,\n",
        "             epochs=100,\n",
        "             batch_size= 64,\n",
        "             validation_data= (X_test, y_test),\n",
        "             verbose=0,\n",
        "             callbacks= []\n",
        "            )\n",
        "print(time.time()- t0,\" secs\")\n",
        "\n",
        "# Retreive the optimal hyperparameters\n",
        "best_hps= tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Retrieve the best model\n",
        "best_model = tuner.get_best_models(num_models=1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6zXQ4BKyltb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "992be919-0bd2-403a-a1db-996565fdb137"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The hyperparameter search is complete. The optimal number of units in the \n",
            "first densely-connected layer is 128,\n",
            "second layer is 96 \n",
            "\n",
            "and the optimal learning rate for the optimizer\n",
            "is 0.003522554497848391.\n",
            "\n",
            "[]\n",
            "1/1 [==============================] - 0s 450ms/step - loss: 0.0138 - mae: 0.0972 - mse: 0.0138\n",
            "loss:0.013777526095509529 mae: 0.09724077582359314 mse: 0.013777526095509529\n"
          ]
        }
      ],
      "source": [
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units in the \n",
        "first densely-connected layer is {best_hps.get('units_1')},\n",
        "second layer is {best_hps.get('units_2')} \n",
        "\n",
        "and the optimal learning rate for the optimizer\n",
        "is {best_hps.get('learning_rate')}.\n",
        "\"\"\")\n",
        "\n",
        "# Evaluate the best model.\n",
        "print(best_model.metrics_names)\n",
        "loss, mae, mse = best_model.evaluate(X_test, y_test)\n",
        "print(f'loss:{loss} mae: {mae} mse: {mse}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgleViO4yltb"
      },
      "source": [
        "Retrain the model with the optimal hyperparameters from the search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grpGQqekyltb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4daa08d-3c63-475b-a594-05a96cad75d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Using Early stopping, needed only 33 epochs to converge!\n"
          ]
        }
      ],
      "source": [
        "# Define the early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "# Build the model with the optimal hyperparameters\n",
        "tuned_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Train the model with early stopping\n",
        "t00 = time.time()\n",
        "history_tuned = tuned_model.fit(X_train, y_train, \n",
        "                                epochs=200, \n",
        "                                validation_data=(X_test, y_test),\n",
        "                                verbose=0,\n",
        "                                callbacks=[early_stopping])\n",
        "\n",
        "print(\"\\n Using Early stopping, needed only\", len(history_tuned.history['val_mse']), \"epochs to converge!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcnDdrH_yltc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc7add3d-8842-4909-920a-a019d3fb85f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "Training accuracy:  0.682534153591094\n",
            "Test accuracy:  0.3763223878035977\n",
            "Test mean-squared error:  0.14428878272348394\n"
          ]
        }
      ],
      "source": [
        "y_pred_train_tuned= tuned_model.predict(X_train).flatten()\n",
        "y_pred_test_tuned= tuned_model.predict(X_test).flatten()\n",
        "\n",
        "print(\"Training accuracy: \",r2_score(y_train, y_pred_train_tuned))\n",
        "\n",
        "print(\"Test accuracy: \",r2_score(y_test, y_pred_test_tuned))\n",
        "\n",
        "print(\"Test mean-squared error: \",np.sqrt(mean_squared_error(y_test, y_pred_test_tuned)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCfqmoziyltc"
      },
      "source": [
        "From the hyperparameter tuning we got that:\n",
        "\n",
        "The optimal number of units in the densely-connected layers is:\n",
        "- First layer: 160\n",
        "- Second layer: 96\n",
        "\n",
        "and the optimal learning rate for the optimizer\n",
        "is 0.002916446662073991.\n",
        "\n",
        "And using early stopping only 31 epochs were needed to converge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7kS3mE0y5E3"
      },
      "source": [
        "# Creating a model with keras tuner: DataALL"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the data used in the training and testing"
      ],
      "metadata": {
        "id": "Ys2fzEpshIt-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yokaNxKAzERz"
      },
      "outputs": [],
      "source": [
        "# #And deleting all the label data from dataALL (just in case)\n",
        "# dropALL = [\"Masa_VAT_cat\", \"Masa_VAT_g\", \"Vol_VAT\"]\n",
        "# dataALL = dataALL.drop(dropALL, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_TQQ5rKzERz"
      },
      "outputs": [],
      "source": [
        "#DATA ALL NN\n",
        "#Separating the target variables (Masa_VAT_g and Vol_VAT) from the independent features\n",
        "target = [\"Masa_VAT_g\"]\n",
        "X = dataALL.values\n",
        "y = dataAP_g[target].values\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKZVX9UUzER0"
      },
      "outputs": [],
      "source": [
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Now we can use X_train and y_train as your training data\n",
        "# and X_test and y_test as your test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KutDHiqlzER0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab704db-e66f-4471-9983-5a7047219167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(118, 33)\n",
            "(118, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHKAsjTHy5FL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70bf4e4b-c9ec-4f3b-a171-1c5764a96f11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.27.1)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BCBu7j7y5FM"
      },
      "outputs": [],
      "source": [
        "from kerastuner import HyperModel\n",
        "from kerastuner.tuners import RandomSearch, Hyperband\n",
        "import tensorflow as tf\n",
        "import IPython\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkWFgJvGy5FM"
      },
      "outputs": [],
      "source": [
        "class ANNhypermodel(HyperModel):\n",
        "    \n",
        "    def __init__(self, input_shape):\n",
        "        self.input_shape= (33,)\n",
        "        \n",
        "    def build(self, hp):\n",
        "        model= tf.keras.Sequential()\n",
        "        \n",
        "        # Tune the number of units in the first Dense layer\n",
        "        # Defining dense units as a close approx to the original neural network to perform a fair comparision!\n",
        "        \n",
        "        \n",
        "        hp_units_1= hp.Int('units_1', min_value=128, max_value= 160, step=32)\n",
        "        hp_units_2= hp.Int('units_2', min_value=64, max_value= 128, step=32)\n",
        "\n",
        "        model.add(tf.keras.layers.Dense(units=hp_units_1, activation='relu', input_shape= self.input_shape))\n",
        "        model.add(tf.keras.layers.Dense(units=hp_units_2, activation='relu'))\n",
        "        model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "        # Tune the learning rate for the optimizer \n",
        "        hp_learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default= 0.0005)\n",
        "\n",
        "        model.compile(loss='mse',\n",
        "                    optimizer= tf.keras.optimizers.Adam(learning_rate=hp_learning_rate), metrics= ['mae','mse']\n",
        "                     )\n",
        "\n",
        "        return model\n",
        "\n",
        "hypermodel= ANNhypermodel(input_shape= [len(X_train)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOarxA3dy5FM"
      },
      "source": [
        "2. Instantiate the tuner to perform hypertuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05f6VS2Ky5FN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6df9dbf8-a42a-4f62-c5c5-2f4c06e6762e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 3\n",
            "units_1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 128, 'max_value': 160, 'step': 32, 'sampling': 'linear'}\n",
            "units_2 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
            "learning_rate (Float)\n",
            "{'default': 0.0005, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
          ]
        }
      ],
      "source": [
        "MAX_TRIALS = 20\n",
        "EXECUTION_PER_TRIAL = 2\n",
        "tuner= RandomSearch(hypermodel,\n",
        "               objective= 'val_mse',\n",
        "               max_trials= MAX_TRIALS,\n",
        "               executions_per_trial= EXECUTION_PER_TRIAL,\n",
        "               directory= 'random_search',\n",
        "               project_name='houseprices',\n",
        "               overwrite=True)\n",
        "\n",
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjswQp9-y5FN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcb8c95f-9d9c-4820-ad1d-3dfaf93e57f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 3\n",
            "units_1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 128, 'max_value': 160, 'step': 32, 'sampling': 'linear'}\n",
            "units_2 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
            "learning_rate (Float)\n",
            "{'default': 0.0005, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
          ]
        }
      ],
      "source": [
        "HYPERBAND_MAX_EPOCHS = 150\n",
        "EXECUTION_PER_TRIAL = 2\n",
        "\n",
        "tuner= Hyperband(hypermodel,\n",
        "                   objective= 'val_mse',\n",
        "                   max_epochs=HYPERBAND_MAX_EPOCHS, #Set 100+ for good results\n",
        "                   executions_per_trial=EXECUTION_PER_TRIAL,\n",
        "                   directory= 'hyperband',\n",
        "                   project_name='houseprices',\n",
        "                   overwrite=True)\n",
        "\n",
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDBvM39iy5FN"
      },
      "source": [
        "Run the hyperparameter search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6abe8kCy5FO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4da4946e-4708-40f4-8272-c5a375bbbd87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "searching for the best params!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n",
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1382.2369079589844  secs\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "print('searching for the best params!')\n",
        "\n",
        "t0= time.time()\n",
        "tuner.search(x= X_train,\n",
        "             y= y_train,\n",
        "             epochs=100,\n",
        "             batch_size= 64,\n",
        "             validation_data= (X_test, y_test),\n",
        "             verbose=0,\n",
        "             callbacks= []\n",
        "            )\n",
        "print(time.time()- t0,\" secs\")\n",
        "\n",
        "# Retreive the optimal hyperparameters\n",
        "best_hps= tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Retrieve the best model\n",
        "best_model = tuner.get_best_models(num_models=1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sfXmVx8y5FO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0568da23-e295-414c-bb98-c93eb417512a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The hyperparameter search is complete. The optimal number of units in the \n",
            "first densely-connected layer is 160,\n",
            "second layer is 128 \n",
            "\n",
            "and the optimal learning rate for the optimizer\n",
            "is 0.002015368564870385.\n",
            "\n",
            "[]\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.0099 - mae: 0.0787 - mse: 0.0099\n",
            "loss:0.009862139821052551 mae: 0.07870952039957047 mse: 0.009862139821052551\n"
          ]
        }
      ],
      "source": [
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units in the \n",
        "first densely-connected layer is {best_hps.get('units_1')},\n",
        "second layer is {best_hps.get('units_2')} \n",
        "\n",
        "and the optimal learning rate for the optimizer\n",
        "is {best_hps.get('learning_rate')}.\n",
        "\"\"\")\n",
        "\n",
        "# Evaluate the best model.\n",
        "print(best_model.metrics_names)\n",
        "loss, mae, mse = best_model.evaluate(X_test, y_test)\n",
        "print(f'loss:{loss} mae: {mae} mse: {mse}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uiix6Ld4y5FO"
      },
      "source": [
        "Retrain the model with the optimal hyperparameters from the search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFcA9m_qy5FP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0374585d-6f1e-46e3-dffb-fa6b3f5b3f6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Using Early stopping, needed only 18 epochs to converge!\n"
          ]
        }
      ],
      "source": [
        "# Define the early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "# Build the model with the optimal hyperparameters\n",
        "tuned_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Train the model with early stopping\n",
        "t00 = time.time()\n",
        "history_tuned = tuned_model.fit(X_train, y_train, \n",
        "                                epochs=200, \n",
        "                                validation_data=(X_test, y_test),\n",
        "                                verbose=0,\n",
        "                                callbacks=[early_stopping])\n",
        "\n",
        "print(\"\\n Using Early stopping, needed only\", len(history_tuned.history['val_mse']), \"epochs to converge!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IucQZgAy5FP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40df2879-44ea-4326-848d-b696f37b549f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "Training accuracy:  0.6992380425105211\n",
            "Test accuracy:  0.4837337354138861\n",
            "Test mean-squared error:  0.13127721831360378\n"
          ]
        }
      ],
      "source": [
        "y_pred_train_tuned= tuned_model.predict(X_train).flatten()\n",
        "y_pred_test_tuned= tuned_model.predict(X_test).flatten()\n",
        "\n",
        "print(\"Training accuracy: \",r2_score(y_train, y_pred_train_tuned))\n",
        "\n",
        "print(\"Test accuracy: \",r2_score(y_test, y_pred_test_tuned))\n",
        "\n",
        "print(\"Test mean-squared error: \",np.sqrt(mean_squared_error(y_test, y_pred_test_tuned)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1clCo0GRy5FP"
      },
      "source": [
        "From the hyperparameter tuning we got that:\n",
        "\n",
        "The optimal number of units in the densely-connected layers is:\n",
        "- First layer: 160\n",
        "- Second layer: 96\n",
        "\n",
        "and the optimal learning rate for the optimizer\n",
        "is 0.0015324287478969592.\n",
        "\n",
        "And using early stopping only 21 epochs were needed to converge."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMJd73fZ0GObsYyLagYiUqC"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}