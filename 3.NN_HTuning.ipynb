{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network hyperparameter tuning "
      ],
      "metadata": {
        "id": "ldUfk8ujgArb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section of the project the NN will be defined with their optimal hyperparameters using Keras Tuner (https://keras.io/keras_tuner/).\n",
        "There will be one neural network for each subset of features (DataAP, dataPHY and dataALL)"
      ],
      "metadata": {
        "id": "AMn81UhUgNFX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9NxpD1fJiy9"
      },
      "source": [
        "## Loading the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tjk9ZGgWGnlg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import pickle as pk\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coZ-triGJow-"
      },
      "source": [
        "## Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6tjVE5BAGbxc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7af65407-f7e1-41f0-b5c1-a482397f211b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVFgplJbcQKs"
      },
      "source": [
        "Importing the dataframes dataAP and dataPHY:\n",
        "\n",
        "- dataAP: data containing the original variables that best correlate with the label data of the dataframe.\n",
        "\n",
        "- dataPHY: data containing variables from the blood test that are of interest.\n",
        "\n",
        "- dataALL: Both dataAP and dataPHY combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tdSuIyrHf1_K"
      },
      "outputs": [],
      "source": [
        "#Importing dataAP\n",
        "file_name = '/content/drive/MyDrive/TFG/dataAP.csv'\n",
        "dataAP = pd.read_csv(file_name, index_col=[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DPMXUik7THTH"
      },
      "outputs": [],
      "source": [
        "#Importing dataPHY\n",
        "file_name = '/content/drive/MyDrive/TFG/dataPHY.csv'\n",
        "dataPHY = pd.read_csv(file_name, index_col=[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Djwv3OgzsbDq"
      },
      "outputs": [],
      "source": [
        "#Importing dataALL\n",
        "file_name = '/content/drive/MyDrive/TFG/dataALL.csv'\n",
        "dataALL = pd.read_csv(file_name, index_col=[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4851cmvA1CZ"
      },
      "source": [
        "One dataframe (**dataAP_g**) will contain the label data in continuous form (Masa_VAT_g) and the other (**dataAP_cat**) will contain the label in\n",
        "categorical form (Masa_VAT_cat), as stated in the \"*Transorming the label data into categotical data*\" section from the preprocessing. This is done to avoid confusion when selecting the target variable (Masa_VAT_cat).\n",
        "\n",
        "Also the label Vol_VAT will be removed as it is technically the same as Masa_VAT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nBqWTiOiP5_G"
      },
      "outputs": [],
      "source": [
        "#Creating a dataAP dataframe only with Masa_VAT_g and another one with only Masa_VAT_cat\n",
        "\n",
        "dataAP_g = dataAP.copy()\n",
        "dataAP_cat = dataAP.copy()\n",
        "\n",
        "dropcat = [\"Masa_VAT_g\", \"Vol_VAT\"]\n",
        "dropg = [\"Masa_VAT_cat\", \"Vol_VAT\"]\n",
        "dataAP_g = dataAP_g.drop(dropg, axis=1)\n",
        "dataAP_cat = dataAP_cat.drop(dropcat , axis=1)\n",
        "\n",
        "#And deleting all the label data from dataALL\n",
        "dropALL = [\"Masa_VAT_cat\", \"Masa_VAT_g\", \"Vol_VAT\"]\n",
        "dataALL = dataALL.drop(dropALL, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFSaiI7vKO0Y"
      },
      "source": [
        "Separating the features X from the label y, next we have the three possible X and y depending on the database we're going to work with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HYoO7xYqKUsT"
      },
      "outputs": [],
      "source": [
        "#DATA AP NN\n",
        "#Separating the target variables (Masa_VAT_g and Vol_VAT) from the independent features\n",
        "target = [\"Masa_VAT_g\"]\n",
        "X = dataAP_g.drop(target, axis=1).values\n",
        "y = dataAP_g[target].values\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Hsz8LGMlcIJT"
      },
      "outputs": [],
      "source": [
        "#DATA PHY NN\n",
        "#Separating the target variables (Masa_VAT_g and Vol_VAT) from the independent features\n",
        "target = [\"Masa_VAT_g\"]\n",
        "X = dataPHY.values\n",
        "y = dataAP_g[target].values\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XhWwiCCKggDP"
      },
      "outputs": [],
      "source": [
        "#DATA ALL NN\n",
        "#Separating the target variables (Masa_VAT_g and Vol_VAT) from the independent features\n",
        "target = [\"Masa_VAT_g\"]\n",
        "X = dataALL.values\n",
        "y = dataAP_g[target].values\t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqGXLdK9Kofh"
      },
      "source": [
        "Train-Validation-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NJLsA57gKpDi"
      },
      "outputs": [],
      "source": [
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Now we can use X_train and y_train as your training data\n",
        "# and X_test and y_test as your test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pissLm6JKwle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32c7a4d1-58b0-4f4c-8ac6-6404f6e61acf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(118, 33)\n",
            "(118, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zE99rG20rFP"
      },
      "source": [
        "# Creating a model with keras tuner: DataAP"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the data used in the training and testing"
      ],
      "metadata": {
        "id": "KxdbctoKg4Aj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "C3tM8cIhye-e"
      },
      "outputs": [],
      "source": [
        "## Data AP\n",
        "#Separating the target variables (Masa_VAT_g and Vol_VAT) from the independent features\n",
        "target = [\"Masa_VAT_g\"]\n",
        "X = dataAP_g.drop(target, axis=1).values\n",
        "y = dataAP_g[target].values\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZAbgU2jzye-p"
      },
      "outputs": [],
      "source": [
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Now we can use X_train and y_train as your training data\n",
        "# and X_test and y_test as your test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "in0HByyiye-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffafae1a-9b8f-47f6-8d31-b01a0b4e748f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(118, 17)\n",
            "(118, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY56GaBDDUnj"
      },
      "source": [
        "Installing keras tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AR_NCnY81Ilv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e329b5c3-eb93-4d69-ac46-797290c96211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.3.5-py3-none-any.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.27.1)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.3.5 kt-legacy-1.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PaIn3A1p1QW3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8813ea53-038b-43df-d674-aeed5e33d9b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-091a60f0002a>:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  from kerastuner import HyperModel\n"
          ]
        }
      ],
      "source": [
        "from kerastuner import HyperModel\n",
        "from kerastuner.tuners import RandomSearch, Hyperband\n",
        "import tensorflow as tf\n",
        "import IPython\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "csJ8RakQ1WTS"
      },
      "outputs": [],
      "source": [
        "class ANNhypermodel(HyperModel):\n",
        "    \n",
        "    def __init__(self, input_shape):\n",
        "        self.input_shape= (17,)\n",
        "        \n",
        "    def build(self, hp):\n",
        "        model= tf.keras.Sequential()\n",
        "        \n",
        "        # Tune the number of units in the first Dense layer\n",
        "        # Defining dense units as a close approx to the original neural network to perform a fair comparision!\n",
        "        \n",
        "        \n",
        "        hp_units_1= hp.Int('units_1', min_value=128, max_value= 160, step=32)\n",
        "        hp_units_2= hp.Int('units_2', min_value=64, max_value= 128, step=32)\n",
        "\n",
        "        model.add(tf.keras.layers.Dense(units=hp_units_1, activation='relu', input_shape= self.input_shape))\n",
        "        model.add(tf.keras.layers.Dense(units=hp_units_2, activation='relu'))\n",
        "        model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "        # Tune the learning rate for the optimizer \n",
        "        hp_learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default= 0.0005)\n",
        "\n",
        "        model.compile(loss='mse',\n",
        "                    optimizer= tf.keras.optimizers.Adam(learning_rate=hp_learning_rate), metrics= ['mae','mse']\n",
        "                     )\n",
        "\n",
        "        return model\n",
        "\n",
        "hypermodel= ANNhypermodel(input_shape= [len(X_train)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7avhshkV1zkv"
      },
      "source": [
        "2. Instantiate the tuner to perform hypertuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "U23iJS2F1zT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcf4bc01-92a8-4640-bb7c-d8c9f6ada61c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 3\n",
            "units_1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 128, 'max_value': 160, 'step': 32, 'sampling': 'linear'}\n",
            "units_2 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
            "learning_rate (Float)\n",
            "{'default': 0.0005, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
          ]
        }
      ],
      "source": [
        "MAX_TRIALS = 20\n",
        "EXECUTION_PER_TRIAL = 2\n",
        "tuner= RandomSearch(hypermodel,\n",
        "               objective= 'val_mse',\n",
        "               max_trials= MAX_TRIALS,\n",
        "               executions_per_trial= EXECUTION_PER_TRIAL,\n",
        "               directory= 'random_search',\n",
        "               project_name='houseprices',\n",
        "               overwrite=True)\n",
        "\n",
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5auRm8fh2WJO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b58397f-a6ca-401a-931a-7566b2c8c4af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 3\n",
            "units_1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 128, 'max_value': 160, 'step': 32, 'sampling': 'linear'}\n",
            "units_2 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
            "learning_rate (Float)\n",
            "{'default': 0.0005, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
          ]
        }
      ],
      "source": [
        "HYPERBAND_MAX_EPOCHS = 150\n",
        "EXECUTION_PER_TRIAL = 2\n",
        "\n",
        "tuner= Hyperband(hypermodel,\n",
        "                   objective= 'val_mse',\n",
        "                   max_epochs=HYPERBAND_MAX_EPOCHS, #Set 100+ for good results\n",
        "                   executions_per_trial=EXECUTION_PER_TRIAL,\n",
        "                   directory= 'hyperband',\n",
        "                   project_name='houseprices',\n",
        "                   overwrite=True)\n",
        "\n",
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfEYafbQ2krn"
      },
      "source": [
        "Run the hyperparameter search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rb-lA2NU2ptn",
        "outputId": "15a2eeb1-e3aa-42e9-a92c-3661cb0b0836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "searching for the best params!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2372eb16c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2372ef92d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1349.1612300872803  secs\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "print('searching for the best params!')\n",
        "\n",
        "t0= time.time()\n",
        "tuner.search(x= X_train,\n",
        "             y= y_train,\n",
        "             epochs=100,\n",
        "             batch_size= 64,\n",
        "             validation_data= (X_test, y_test),\n",
        "             verbose=0,\n",
        "             callbacks= []\n",
        "            )\n",
        "print(time.time()- t0,\" secs\")\n",
        "\n",
        "# Retreive the optimal hyperparameters\n",
        "best_hps= tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Retrieve the best model\n",
        "best_model = tuner.get_best_models(num_models=1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bZdURQwm3_s3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cfbf402-9284-44d5-faea-b3ae58033dde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The hyperparameter search is complete. The optimal number of units in the \n",
            "first densely-connected layer is 160,\n",
            "second layer is 96 \n",
            "\n",
            "and the optimal learning rate for the optimizer\n",
            "is 0.0076208407593745765.\n",
            "\n",
            "[]\n",
            "1/1 [==============================] - 0s 353ms/step - loss: 0.0202 - mae: 0.1153 - mse: 0.0202\n",
            "loss:0.020223401486873627 mae: 0.11528563499450684 mse: 0.020223401486873627\n"
          ]
        }
      ],
      "source": [
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units in the \n",
        "first densely-connected layer is {best_hps.get('units_1')},\n",
        "second layer is {best_hps.get('units_2')} \n",
        "\n",
        "and the optimal learning rate for the optimizer\n",
        "is {best_hps.get('learning_rate')}.\n",
        "\"\"\")\n",
        "\n",
        "# Evaluate the best model.\n",
        "print(best_model.metrics_names)\n",
        "loss, mae, mse = best_model.evaluate(X_test, y_test)\n",
        "print(f'loss:{loss} mae: {mae} mse: {mse}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_waz7g45EbM"
      },
      "source": [
        "Retrain the model with the optimal hyperparameters from the search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ocL8HSg35Fii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04969a06-72d8-499c-dc73-3c33f1ffe848"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Using Early stopping, needed only 14 epochs to converge!\n"
          ]
        }
      ],
      "source": [
        "# Define the early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "# Build the model with the optimal hyperparameters\n",
        "tuned_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Train the model with early stopping\n",
        "t00 = time.time()\n",
        "history_tuned = tuned_model.fit(X_train, y_train, \n",
        "                                epochs=200, \n",
        "                                validation_data=(X_test, y_test),\n",
        "                                verbose=0,\n",
        "                                callbacks=[early_stopping])\n",
        "\n",
        "print(\"\\n Using Early stopping, needed only\", len(history_tuned.history['val_mse']), \"epochs to converge!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ii8vNrMg5-8O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8723edf2-4dc3-4c32-e53d-f3b46a06c08a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Training accuracy:  0.28809270326799796\n",
            "Test accuracy:  0.24579536576063976\n",
            "Test mean-squared error:  0.15867082889883857\n"
          ]
        }
      ],
      "source": [
        "y_pred_train_tuned= tuned_model.predict(X_train).flatten()\n",
        "y_pred_test_tuned= tuned_model.predict(X_test).flatten()\n",
        "\n",
        "print(\"Training accuracy: \",r2_score(y_train, y_pred_train_tuned))\n",
        "\n",
        "print(\"Test accuracy: \",r2_score(y_test, y_pred_test_tuned))\n",
        "\n",
        "print(\"Test mean-squared error: \",np.sqrt(mean_squared_error(y_test, y_pred_test_tuned)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2aZvqRcqCc6"
      },
      "source": [
        "From the hyperparameter tuning we got that:\n",
        "\n",
        "The optimal number of units in the densely-connected layers is:\n",
        "- First layer: 160\n",
        "- Second layer: 96\n",
        "\n",
        "and the optimal learning rate for the optimizer\n",
        "is 0.0076208407593745765.\n",
        "\n",
        "And using early stopping only 14 epochs were needed to converge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVJUegYayltZ"
      },
      "source": [
        "# Creating a model with keras tuner: DataPHY"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the data used in the training and testing"
      ],
      "metadata": {
        "id": "jO9PLmuchEeO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-T7-aClpyucX"
      },
      "outputs": [],
      "source": [
        "#DATA PHY NN\n",
        "#Separating the target variables (Masa_VAT_g and Vol_VAT) from the independent features\n",
        "target = [\"Masa_VAT_g\"]\n",
        "X = dataPHY.values\n",
        "y = dataAP_g[target].values\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zDl2ozVNyucg"
      },
      "outputs": [],
      "source": [
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Now we can use X_train and y_train as your training data\n",
        "# and X_test and y_test as your test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xqRgwLZFyuch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ce474c3-6528-4dc4-d1b8-d1aa8737b44c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(118, 16)\n",
            "(118, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "F3qGFWwkylta",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9b880a5-11a3-4177-a2c5-8ff83dfefc7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.27.1)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "DP_d7pcvylta"
      },
      "outputs": [],
      "source": [
        "from kerastuner import HyperModel\n",
        "from kerastuner.tuners import RandomSearch, Hyperband\n",
        "import tensorflow as tf\n",
        "import IPython\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "YNDDiFKsylta"
      },
      "outputs": [],
      "source": [
        "class ANNhypermodel(HyperModel):\n",
        "    \n",
        "    def __init__(self, input_shape):\n",
        "        self.input_shape= (16,)\n",
        "        \n",
        "    def build(self, hp):\n",
        "        model= tf.keras.Sequential()\n",
        "        \n",
        "        # Tune the number of units in the first Dense layer\n",
        "        # Defining dense units as a close approx to the original neural network to perform a fair comparision!\n",
        "        \n",
        "        \n",
        "        hp_units_1= hp.Int('units_1', min_value=128, max_value= 160, step=32)\n",
        "        hp_units_2= hp.Int('units_2', min_value=64, max_value= 128, step=32)\n",
        "\n",
        "        model.add(tf.keras.layers.Dense(units=hp_units_1, activation='relu', input_shape= self.input_shape))\n",
        "        model.add(tf.keras.layers.Dense(units=hp_units_2, activation='relu'))\n",
        "        model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "        # Tune the learning rate for the optimizer \n",
        "        hp_learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default= 0.0005)\n",
        "\n",
        "        model.compile(loss='mse',\n",
        "                    optimizer= tf.keras.optimizers.Adam(learning_rate=hp_learning_rate), metrics= ['mae','mse']\n",
        "                     )\n",
        "\n",
        "        return model\n",
        "\n",
        "hypermodel= ANNhypermodel(input_shape= [len(X_train)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50-A9pK0yltb"
      },
      "source": [
        "2. Instantiate the tuner to perform hypertuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "6SwAD4Ccyltb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cad299f4-f65a-41cd-f45d-11e7279c70cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 3\n",
            "units_1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 128, 'max_value': 160, 'step': 32, 'sampling': 'linear'}\n",
            "units_2 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
            "learning_rate (Float)\n",
            "{'default': 0.0005, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
          ]
        }
      ],
      "source": [
        "MAX_TRIALS = 20\n",
        "EXECUTION_PER_TRIAL = 2\n",
        "tuner= RandomSearch(hypermodel,\n",
        "               objective= 'val_mse',\n",
        "               max_trials= MAX_TRIALS,\n",
        "               executions_per_trial= EXECUTION_PER_TRIAL,\n",
        "               directory= 'random_search',\n",
        "               project_name='houseprices',\n",
        "               overwrite=True)\n",
        "\n",
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "sJ6osdpGyltb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b6878c7-9287-471d-a84e-769f5157aed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 3\n",
            "units_1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 128, 'max_value': 160, 'step': 32, 'sampling': 'linear'}\n",
            "units_2 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
            "learning_rate (Float)\n",
            "{'default': 0.0005, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
          ]
        }
      ],
      "source": [
        "HYPERBAND_MAX_EPOCHS = 150\n",
        "EXECUTION_PER_TRIAL = 2\n",
        "\n",
        "tuner= Hyperband(hypermodel,\n",
        "                   objective= 'val_mse',\n",
        "                   max_epochs=HYPERBAND_MAX_EPOCHS, #Set 100+ for good results\n",
        "                   executions_per_trial=EXECUTION_PER_TRIAL,\n",
        "                   directory= 'hyperband',\n",
        "                   project_name='houseprices',\n",
        "                   overwrite=True)\n",
        "\n",
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt3Tx9Yayltb"
      },
      "source": [
        "Run the hyperparameter search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Mn04o6WPyltb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c80ac38-338d-412d-d926-2f55a2a613a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "searching for the best params!\n",
            "1650.080234527588  secs\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "print('searching for the best params!')\n",
        "\n",
        "t0= time.time()\n",
        "tuner.search(x= X_train,\n",
        "             y= y_train,\n",
        "             epochs=100,\n",
        "             batch_size= 64,\n",
        "             validation_data= (X_test, y_test),\n",
        "             verbose=0,\n",
        "             callbacks= []\n",
        "            )\n",
        "print(time.time()- t0,\" secs\")\n",
        "\n",
        "# Retreive the optimal hyperparameters\n",
        "best_hps= tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Retrieve the best model\n",
        "best_model = tuner.get_best_models(num_models=1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "u6zXQ4BKyltb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37261549-2ad1-47f3-dc2d-8bc30e66d63f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The hyperparameter search is complete. The optimal number of units in the \n",
            "first densely-connected layer is 160,\n",
            "second layer is 64 \n",
            "\n",
            "and the optimal learning rate for the optimizer\n",
            "is 0.006528791394798441.\n",
            "\n",
            "[]\n",
            "1/1 [==============================] - 1s 656ms/step - loss: 0.0129 - mae: 0.0941 - mse: 0.0129\n",
            "loss:0.012929310090839863 mae: 0.09407304972410202 mse: 0.012929310090839863\n"
          ]
        }
      ],
      "source": [
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units in the \n",
        "first densely-connected layer is {best_hps.get('units_1')},\n",
        "second layer is {best_hps.get('units_2')} \n",
        "\n",
        "and the optimal learning rate for the optimizer\n",
        "is {best_hps.get('learning_rate')}.\n",
        "\"\"\")\n",
        "\n",
        "# Evaluate the best model.\n",
        "print(best_model.metrics_names)\n",
        "loss, mae, mse = best_model.evaluate(X_test, y_test)\n",
        "print(f'loss:{loss} mae: {mae} mse: {mse}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgleViO4yltb"
      },
      "source": [
        "Retrain the model with the optimal hyperparameters from the search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "grpGQqekyltb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6c164b0-3e33-417b-9427-9904cec30841"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Using Early stopping, needed only 12 epochs to converge!\n"
          ]
        }
      ],
      "source": [
        "# Define the early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "# Build the model with the optimal hyperparameters\n",
        "tuned_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Train the model with early stopping\n",
        "t00 = time.time()\n",
        "history_tuned = tuned_model.fit(X_train, y_train, \n",
        "                                epochs=200, \n",
        "                                validation_data=(X_test, y_test),\n",
        "                                verbose=0,\n",
        "                                callbacks=[early_stopping])\n",
        "\n",
        "print(\"\\n Using Early stopping, needed only\", len(history_tuned.history['val_mse']), \"epochs to converge!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "tcnDdrH_yltc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "470a2d30-8b49-4a08-df27-9b53096a672e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Training accuracy:  0.37088301841259097\n",
            "Test accuracy:  0.4068863020834368\n",
            "Test mean-squared error:  0.14070886803514687\n"
          ]
        }
      ],
      "source": [
        "y_pred_train_tuned= tuned_model.predict(X_train).flatten()\n",
        "y_pred_test_tuned= tuned_model.predict(X_test).flatten()\n",
        "\n",
        "print(\"Training accuracy: \",r2_score(y_train, y_pred_train_tuned))\n",
        "\n",
        "print(\"Test accuracy: \",r2_score(y_test, y_pred_test_tuned))\n",
        "\n",
        "print(\"Test mean-squared error: \",np.sqrt(mean_squared_error(y_test, y_pred_test_tuned)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCfqmoziyltc"
      },
      "source": [
        "From the hyperparameter tuning we got that:\n",
        "\n",
        "The optimal number of units in the densely-connected layers is:\n",
        "- First layer: 160\n",
        "- Second layer: 64\n",
        "\n",
        "and the optimal learning rate for the optimizer\n",
        "is 0.006528791394798441.\n",
        "\n",
        "And using early stopping only 12 epochs were needed to converge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7kS3mE0y5E3"
      },
      "source": [
        "# Creating a model with keras tuner: DataALL"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the data used in the training and testing"
      ],
      "metadata": {
        "id": "Ys2fzEpshIt-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "yokaNxKAzERz"
      },
      "outputs": [],
      "source": [
        "# #And deleting all the label data from dataALL (just in case)\n",
        "# dropALL = [\"Masa_VAT_cat\", \"Masa_VAT_g\", \"Vol_VAT\"]\n",
        "# dataALL = dataALL.drop(dropALL, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "V_TQQ5rKzERz"
      },
      "outputs": [],
      "source": [
        "#DATA ALL NN\n",
        "#Separating the target variables (Masa_VAT_g and Vol_VAT) from the independent features\n",
        "target = [\"Masa_VAT_g\"]\n",
        "X = dataALL.values\n",
        "y = dataAP_g[target].values\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "UKZVX9UUzER0"
      },
      "outputs": [],
      "source": [
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Now we can use X_train and y_train as your training data\n",
        "# and X_test and y_test as your test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "KutDHiqlzER0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5922a3e-7be2-4404-930f-a6bd8487cbbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(118, 33)\n",
            "(118, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "dHKAsjTHy5FL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c57a7e-f72a-4fdd-b8f3-c9084a16248b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.27.1)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "-BCBu7j7y5FM"
      },
      "outputs": [],
      "source": [
        "from kerastuner import HyperModel\n",
        "from kerastuner.tuners import RandomSearch, Hyperband\n",
        "import tensorflow as tf\n",
        "import IPython\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "SkWFgJvGy5FM"
      },
      "outputs": [],
      "source": [
        "class ANNhypermodel(HyperModel):\n",
        "    \n",
        "    def __init__(self, input_shape):\n",
        "        self.input_shape= (33,)\n",
        "        \n",
        "    def build(self, hp):\n",
        "        model= tf.keras.Sequential()\n",
        "        \n",
        "        # Tune the number of units in the first Dense layer\n",
        "        # Defining dense units as a close approx to the original neural network to perform a fair comparision!\n",
        "        \n",
        "        \n",
        "        hp_units_1= hp.Int('units_1', min_value=128, max_value= 160, step=32)\n",
        "        hp_units_2= hp.Int('units_2', min_value=64, max_value= 128, step=32)\n",
        "\n",
        "        model.add(tf.keras.layers.Dense(units=hp_units_1, activation='relu', input_shape= self.input_shape))\n",
        "        model.add(tf.keras.layers.Dense(units=hp_units_2, activation='relu'))\n",
        "        model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "        # Tune the learning rate for the optimizer \n",
        "        hp_learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default= 0.0005)\n",
        "\n",
        "        model.compile(loss='mse',\n",
        "                    optimizer= tf.keras.optimizers.Adam(learning_rate=hp_learning_rate), metrics= ['mae','mse']\n",
        "                     )\n",
        "\n",
        "        return model\n",
        "\n",
        "hypermodel= ANNhypermodel(input_shape= [len(X_train)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOarxA3dy5FM"
      },
      "source": [
        "2. Instantiate the tuner to perform hypertuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "05f6VS2Ky5FN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f68e57c3-24f0-4456-8ea1-b6eeeebc3c98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 3\n",
            "units_1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 128, 'max_value': 160, 'step': 32, 'sampling': 'linear'}\n",
            "units_2 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
            "learning_rate (Float)\n",
            "{'default': 0.0005, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
          ]
        }
      ],
      "source": [
        "MAX_TRIALS = 20\n",
        "EXECUTION_PER_TRIAL = 2\n",
        "tuner= RandomSearch(hypermodel,\n",
        "               objective= 'val_mse',\n",
        "               max_trials= MAX_TRIALS,\n",
        "               executions_per_trial= EXECUTION_PER_TRIAL,\n",
        "               directory= 'random_search',\n",
        "               project_name='houseprices',\n",
        "               overwrite=True)\n",
        "\n",
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "sjswQp9-y5FN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36bcbf7f-c92e-4de1-d8ea-6a20dc4d6038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 3\n",
            "units_1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 128, 'max_value': 160, 'step': 32, 'sampling': 'linear'}\n",
            "units_2 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
            "learning_rate (Float)\n",
            "{'default': 0.0005, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
          ]
        }
      ],
      "source": [
        "HYPERBAND_MAX_EPOCHS = 150\n",
        "EXECUTION_PER_TRIAL = 2\n",
        "\n",
        "tuner= Hyperband(hypermodel,\n",
        "                   objective= 'val_mse',\n",
        "                   max_epochs=HYPERBAND_MAX_EPOCHS, #Set 100+ for good results\n",
        "                   executions_per_trial=EXECUTION_PER_TRIAL,\n",
        "                   directory= 'hyperband',\n",
        "                   project_name='houseprices',\n",
        "                   overwrite=True)\n",
        "\n",
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDBvM39iy5FN"
      },
      "source": [
        "Run the hyperparameter search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "N6abe8kCy5FO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e76173a-8cd8-45af-d80b-170cb61c436e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "searching for the best params!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n",
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1893.022055864334  secs\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "print('searching for the best params!')\n",
        "\n",
        "t0= time.time()\n",
        "tuner.search(x= X_train,\n",
        "             y= y_train,\n",
        "             epochs=100,\n",
        "             batch_size= 64,\n",
        "             validation_data= (X_test, y_test),\n",
        "             verbose=0,\n",
        "             callbacks= []\n",
        "            )\n",
        "print(time.time()- t0,\" secs\")\n",
        "\n",
        "# Retreive the optimal hyperparameters\n",
        "best_hps= tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Retrieve the best model\n",
        "best_model = tuner.get_best_models(num_models=1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "_sfXmVx8y5FO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "470e731c-f334-4067-d574-036f0889ef66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The hyperparameter search is complete. The optimal number of units in the \n",
            "first densely-connected layer is 128,\n",
            "second layer is 128 \n",
            "\n",
            "and the optimal learning rate for the optimizer\n",
            "is 0.004386943776968041.\n",
            "\n",
            "[]\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.0110 - mae: 0.0826 - mse: 0.0110\n",
            "loss:0.010983134619891644 mae: 0.08261366933584213 mse: 0.010983134619891644\n"
          ]
        }
      ],
      "source": [
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units in the \n",
        "first densely-connected layer is {best_hps.get('units_1')},\n",
        "second layer is {best_hps.get('units_2')} \n",
        "\n",
        "and the optimal learning rate for the optimizer\n",
        "is {best_hps.get('learning_rate')}.\n",
        "\"\"\")\n",
        "\n",
        "# Evaluate the best model.\n",
        "print(best_model.metrics_names)\n",
        "loss, mae, mse = best_model.evaluate(X_test, y_test)\n",
        "print(f'loss:{loss} mae: {mae} mse: {mse}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uiix6Ld4y5FO"
      },
      "source": [
        "Retrain the model with the optimal hyperparameters from the search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "CFcA9m_qy5FP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8659b6aa-b792-4ffc-b09d-c111e700dc8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Using Early stopping, needed only 45 epochs to converge!\n"
          ]
        }
      ],
      "source": [
        "# Define the early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "# Build the model with the optimal hyperparameters\n",
        "tuned_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Train the model with early stopping\n",
        "t00 = time.time()\n",
        "history_tuned = tuned_model.fit(X_train, y_train, \n",
        "                                epochs=200, \n",
        "                                validation_data=(X_test, y_test),\n",
        "                                verbose=0,\n",
        "                                callbacks=[early_stopping])\n",
        "\n",
        "print(\"\\n Using Early stopping, needed only\", len(history_tuned.history['val_mse']), \"epochs to converge!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "3IucQZgAy5FP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa9af2f3-b1e4-466d-f820-f62178066efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 11ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "Training accuracy:  0.8320683716051002\n",
            "Test accuracy:  0.5591770164508776\n",
            "Test mean-squared error:  0.12130664931971893\n"
          ]
        }
      ],
      "source": [
        "y_pred_train_tuned= tuned_model.predict(X_train).flatten()\n",
        "y_pred_test_tuned= tuned_model.predict(X_test).flatten()\n",
        "\n",
        "print(\"Training accuracy: \",r2_score(y_train, y_pred_train_tuned))\n",
        "\n",
        "print(\"Test accuracy: \",r2_score(y_test, y_pred_test_tuned))\n",
        "\n",
        "print(\"Test mean-squared error: \",np.sqrt(mean_squared_error(y_test, y_pred_test_tuned)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1clCo0GRy5FP"
      },
      "source": [
        "From the hyperparameter tuning we got that:\n",
        "\n",
        "The optimal number of units in the densely-connected layers is:\n",
        "- First layer: 128\n",
        "- Second layer: 128\n",
        "\n",
        "and the optimal learning rate for the optimizer\n",
        "is 0.004386943776968041.\n",
        "\n",
        "And using early stopping only 45 epochs were needed to converge."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOdcn5QTSpsMcMHPsnEyKpw"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
